{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/stars_train_new.csv', index_col=0)\n",
    "test = pd.read_csv('data/stars_test_new.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also initialize the random seed to ensure the reproducibility of our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking if the dataset is clean (duplications, missing values, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the covariates are continuous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look to correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by examining the relationships between all the variables to discover any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a strong correlation among the variables $u, g, r, i$ and $z$ as they increase almost linearly, with a larger variation in the higher values, resulting in the observed arrow-like pattern. In this graph, we cannot glean much information about the relationship between the label and the other covariables. Apart from the $\\textit{redshift}$ covariable, they appear to be uniformly distributed. To obtain more information, we prefer to create the same graph with the label assigned as the color of the data points, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train, hue='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph we notice that the points are really $\\textit{separable}$ with the $\\textit{redshift}$ covariable. \n",
    "\n",
    "Now, we represent a heatmap in order to get numerically the correlation already discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed in the previous plots, a strong correlation exists among the variables $u, g, r, i,$ and $z$. Such correlations can be problematic for the learning process. Therefore, in the following steps, we will modify the data to retain the information without unnecessary redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to check if the labels are sufficiently represented in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=train, x='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label 0 is more represented than the others labels. Nevertheless, all the classes are sufficiently represented to be able to learn from them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "for i in range(len(train.columns[:-1])):\n",
    "    sns.boxplot(data=train, y=train.columns[i], ax=axes[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "for i in range(len(train.columns[:-1])):\n",
    "    sns.boxplot(data=train, y=train.columns[i], x='label', ax=axes[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe for example that as shown in the pairplot that for the $\\textit{redshift}$ the first class only takes 0 as a value while the second one is more distributed. Furthermore, as the box of the label 2 is narrower than the others in for the variables $u, g, r, i$ and $z$, it is more concentrated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to learn other non-linear structures in the underlying data to discover if it is helpful for the discrimination task. To do this, we introduce the TSNE with multiple perplexity parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.loc[:, :\"redshift\"]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=seed)\n",
    "projections = tsne.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.scatterplot(\n",
    "   x= projections[:,0],y= projections[:,1],\n",
    "    hue=train[\"label\"]\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "perpl=[5, 15, 30,50]\n",
    "for i in range(len(perpl)):\n",
    "    tsne = TSNE(n_components=2, random_state=seed, perplexity=perpl[i])\n",
    "    projections = tsne.fit_transform(features)\n",
    "    fig = sns.scatterplot(x= projections[:,0],y= projections[:,1], hue=train[\"label\"], ax=axes[i])\n",
    "    axes[i].set_title(f\"Perplexity = {perpl[i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we observe that the points are highly mixed between the classes. Therefore, this dimension reduction method may not be helpful for our classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by splitting the training set into two subsets: a training subset and a testing subset. This allows us to examine the conditional generalization error of our estimate. This initial split is primarily to get an idea of the accuracy, as our main approach for selecting model hyperparameters will involve cross-validation, providing a more accurate estimate of generalization error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train.drop(\"label\", axis= 1)\n",
    "Y=train[\"label\"]\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, Y, stratify= Y, random_state=seed, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_sc=scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have noticed from the correlation map, there are numerous highly correlated features. Consequently, having both of them in our analysis might introduce noise rather than valuable information. To mitigate this issue, we will initiate our preprocessing by applying Principal Component Analysis (PCA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=seed)\n",
    "train_pca=pca.fit_transform(X_train_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the total variance is the number of covariates(8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "for i in range(4):\n",
    "    fig = sns.scatterplot(x= train_pca[:,i],y= train_pca[:,i+1], hue=Y_train, ax=axes[i])\n",
    "    axes[i].set_title(f\"Main axes ({i},{i+1})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that in these representations the labels are really $\\textit{separable}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1,9),pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Principal axes\")\n",
    "plt.title(\"Explained variance ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the first principal axis primarily explains the variance. However, we will select the number of axes to retain in order to explain at least 95% of the variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Variance explained with 4 principal axes: {pca.explained_variance_ratio_[:4].sum()}\")\n",
    "print(f\"Variance explained with 5 principal axes: {pca.explained_variance_ratio_[:5].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we keep the first 5 axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca=train_pca[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply these changes to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca=pca.transform(scaler.transform(X_test))[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to leverage the entire training dataset for a more comprehensive learning experience, rather than just the portion used for the initial split, with the goal of evaluating our model's F1-score. As a result, we perform PCA and Scaling on the complete training dataset, and subsequently apply the same transformations to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_Sc=StandardScaler()\n",
    "X_sc=general_Sc.fit_transform(X)\n",
    "general_pca=PCA(random_state=seed)\n",
    "X_pca=general_pca.fit_transform(X_sc)\n",
    "X_final=X_pca[:,:5]\n",
    "test_trans=general_pca.transform(general_Sc.transform(test))[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with very simple algorithms and continue with more advanced ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal of this project is to aggregate all the classifiers we have introduced, emphasizing diversity among them. This aligns with the idea behind Random Forests, which constructs base learners with minimal correlation to reduce generalization error. Hence, we will aggregate all the results of our intermediate learners and construct and aggregating method that learns from these results as if they where the initial features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "weight=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce an easy function that fits the model and gives the f1 score in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_testing(model, X_train, Y_train, X_test, Y_test):\n",
    "    model.fit(X_train, Y_train)\n",
    "    predictions= model.predict(X_test)\n",
    "    score = f1_score(Y_test, predictions, average='weighted')\n",
    "    print(f\"Weighted F1 score: \\033[91m{score.round(6)}\\033[0m for {model.__class__.__name__}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AggregatePredictions(models, X_train_pca, Y_train, X_test_pca, X_final, Y, test_trans):\n",
    "    agg_train=pd.DataFrame()\n",
    "    agg_test=pd.DataFrame()\n",
    "    for model in models:\n",
    "        model.fit(X_train_pca, Y_train)\n",
    "        agg_train[model.__class__.__name__]=model.predict(X_test_pca)\n",
    "        model.fit(X_final, Y)\n",
    "        agg_test[model.__class__.__name__]=model.predict(test_trans)\n",
    "        # Download predictions\n",
    "        #pd.DataFrame(model.predict(test_trans), index=test.index).to_csv(f\"data/predictions_{model.__class__.__name__}.csv\")\n",
    "\n",
    "        return agg_train, agg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier for multivariate Bernoulli models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the simple model of Naive Bayes Classifier for multivariate Bernoulli models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bern = BernoulliNB()\n",
    "weight.append(learning_testing(bern, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(bern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not give a really good accuracy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try with Gaussian instead of Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = GaussianNB()\n",
    "weight.append(learning_testing(gauss, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have improved a little bit the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the linear classifiers. To do so, we start by assuming that the distributions of $X$ given the label is gaussian, sharing all the classes the same covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis()\n",
    "weight.append(learning_testing(LDA, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to use a logistic regression. It is the classifier that minimizes the logistique pseudo-risk.\n",
    "\n",
    " We recall that it is regularized. Our penalty will be l2 (l1 penalty is more used for variable selection and in this case it is not the goal as we do not have high dimensional data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=seed)\n",
    "weight.append(learning_testing(lr, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we would like to optimize the regulariser parameter. To do so, we apply cross validation. We recall that $C$ is the inverse of the regularization coefficient: larger the value, smaller the regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':[0.5,1,2, 5, 10, 20, 25, 30, 100, 500, 1000]}\n",
    "lr = LogisticRegression(random_state=seed)\n",
    "GSlr = GridSearchCV(lr, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSlr.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSlr.best_params_} with a score of {GSlr.best_score_} for the model {GSlr.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSlr.cv_results_[\"rank_test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have $\\textit{regularized}$ the model by reducing the number of covariates through PCA, so it is advisable to decrease the regularization, and eventually, we remove the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrNR = LogisticRegression(random_state=seed, penalty=None)\n",
    "weight.append(learning_testing(lrNR, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(lrNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we introduce the Perceptron, which is a geometric linear model based on the separability of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"penalty\":[None, \"l2\",\"elasticnet\"], \"alpha\":[0.001, 0.01, 0.1, 1],\"max_iter\":[500, 750, 1000, 1500, 2000]}\n",
    "percep = Perceptron(random_state=seed)\n",
    "GSpercep = GridSearchCV(percep, parameters, scoring=\"f1_weighted\", n_jobs=-1,verbose=3 )\n",
    "GSpercep.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSpercep.best_params_} with a score of {GSpercep.best_score_} for the model {GSpercep.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percep = Perceptron(random_state=seed, alpha=0.001, max_iter=500, penalty=None)\n",
    "weight.append(learning_testing(percep, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(percep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasive-Aggressive Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this family of classifiers it to respond $\\textit{passive}$ for the correct answers and $\\textit{aggressive}$ for the incorrect ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"C\":[0.01, 0.1, 0.5, 1, 5],\"max_iter\":[500, 750, 1000, 1500, 2000]}\n",
    "psag = PassiveAggressiveClassifier(random_state=seed)\n",
    "GSpsag = GridSearchCV(psag, parameters, scoring=\"f1_weighted\", n_jobs=-1,verbose=1)\n",
    "GSpsag.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSpsag.best_params_} with a score of {GSpsag.best_score_} for the model {GSpsag.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psag = PassiveAggressiveClassifier(random_state=seed, C=0.1, max_iter=500)\n",
    "weight.append(learning_testing(psag, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(psag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we observe that we are in low dimensional dataset thanks to our PCA. Therefore, it makes sense to apply algorithms based on distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KN=KNeighborsClassifier()\n",
    "learning_testing(KN, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors':[4, 5, 7, 10, 13, 15, 20, 30, 50, 75]}\n",
    "KN = KNeighborsClassifier()\n",
    "GSKN = GridSearchCV(KN, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSKN.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSKN.best_params_} with a score of {GSKN.best_score_} for the model {GSKN.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KNeighborsClassifier(n_neighbors=5)\n",
    "weight.append(learning_testing(kn, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of assuming the same covariance for all covariate distributions conditioned on the labels, we assume that each one has a different covariance matrix. This makes the decision frontier quadratic, in contrast to the linear decision boundary in LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDA=QuadraticDiscriminantAnalysis()\n",
    "weight.append(learning_testing(QDA, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine (SVM), a method that aims to construct a discriminating hyperplane that maximizes the margins between classes. Once this it is computed, the predictions will be done by taking the sign of the signed distance to the hyperplane. This hyperplane is computed by minimizing the hinge loss, which is given by $$\\Phi(X):=(1-X)_{+},$$\n",
    "where $(x)_+:=\\mathrm{max}(0,x)$. Furthermore, there is also a term that can be interpreted as a Ridge regularisation. Therefore, the SVM can be seen as the following minimisation problem:\n",
    "\\begin{align*}\n",
    "    \\mathrm{min}_{w,b}\\frac{1}{2}\\|w \\|^2+C\\sum_{i=1}^n\\left(1-Y_i(w^\\top X_i+b)\\right)_+,\n",
    "\\end{align*}\n",
    "so that we are penalizing the signed distance to the hyperplane defined by $w$ and $b$. \n",
    "The regularization parameter $C$, works inversely to the regularization strength. A higher value of $C$ leads to increased penalization of misclassified observations or those near the margin. Consequently, this results in a more complex construction of the margin which is less $\\textit{regularized}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, only the points that are too close to the margin or those that are misclassified will have an impact on constructing the separating hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVClin=LinearSVC(random_state=seed, max_iter=2500)\n",
    "learning_testing(SVClin, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \"tol\":[1e-4,1e-5, 1e-6], \"C\":[0.5,1,5,10, 50, 100], \"max_iter\":[750,1000,1500, 2000]}\n",
    "SVClin = LinearSVC(random_state=seed, loss=\"squared_hinge\", multi_class=\"crammer_singer\")\n",
    "GSSVClin= GridSearchCV(SVClin, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSSVClin.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSSVClin.best_params_} with a score of {GSSVClin.best_score_} for the model {GSSVClin.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVClin=LinearSVC(random_state=seed, C=50, max_iter=750, tol=0.0001, loss=\"squared_hinge\", multi_class=\"crammer_singer\")\n",
    "weight.append(learning_testing(SVClin, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(SVClin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we try the polynomial kernel, given by $$K_{\\mathrm{poly}}(x,y):=\\left(\\gamma\\langle x,y\\rangle+c\\right)^d,$$\n",
    " where $d$ represents the degree and $c$ and $\\gamma$ are coefficients controlling the significance of higher-order versus lower-order polynomials. In the Scikit-learn library, the value of the coefficient $c$ can be adjusted using the parameter `coef0`, the value of $d$ can be modified using the parameter `degree`, and the value of $\\gamma$ by `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCpol=SVC(random_state=seed, kernel=\"poly\", degree=3, C=100)\n",
    "learning_testing(SVCpol, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = {\"degree\":[3, 4], \"C\":[1,10, 50, 100]}\n",
    "parameters = {\"degree\":[3], \"C\":[100, 150, 200, 300]}\n",
    "SVCpol = SVC(tol=0.001, kernel=\"poly\", random_state=seed)\n",
    "GSSVCpol = GridSearchCV(SVCpol, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSSVCpol.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSSVCpol.best_params_} with a score of {GSSVCpol.best_score_} for the model {GSSVCpol.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCpol=SVC(tol=0.001, kernel=\"poly\", random_state=seed, degree=3, C=200)\n",
    "weight.append(learning_testing(SVCpol, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(SVCpol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other kernels SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also tried other different kernels such as the Gaussian one, which is usually referred as $\\textit{Radial Basis Function}$(RBF) and which is computed by\n",
    "$$K_{\\mathrm{RBF}}(x,y):=\\mathrm{exp}\\left(-\\gamma\\|x-y\\|^2\\right),$$\n",
    "where the $\\gamma$ can be seen as the inverse of the $2\\sigma^2$ of a Gaussian distribution, so as the $\\gamma$ increases, the variance decreases, causing the kernel to yield smaller values. This parameter can be tuned by the parameter `gamma` in Scikit-learn.\n",
    "\n",
    " Finally, another kernel denoted as Sigmoid is given by\n",
    " $$K_{\\mathrm{sigm}}(x,y):=\\mathrm{tanh}\\left(\\langle x , y\\rangle +r\\right),$$\n",
    "where $\\alpha$ represents the parameter `alpha` and $r$ the parameter `coef0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCkern=SVC(C= 1000, decision_function_shape= 'ovo', gamma= 'auto', kernel='rbf')\n",
    "learning_testing(SVCkern, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = {\"kernel\":[\"rbf\", \"sigmoid\"], \"gamma\":[\"scale\", \"auto\"], \"C\":[3,5,10],\"decision_function_shape\":[\"ovo\", \"ovr\"]}\n",
    "parameters = { \"C\":[10, 50, 100, 500, 1000]}\n",
    "SVCkern = SVC(random_state=seed, decision_function_shape=\"ovo\", gamma=\"auto\", kernel=\"rbf\")\n",
    "GSSVCKern = GridSearchCV(SVCkern, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSSVCKern.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSSVCKern.best_params_} with a score of {GSSVCKern.best_score_} for the model {GSSVCKern.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVCkern=SVC(random_state=seed, decision_function_shape=\"ovo\", gamma=\"auto\", kernel=\"rbf\", C=500)\n",
    "weight.append(learning_testing(SVCkern, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(SVCkern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method attempts to partition the space by posing binary questions, each on a single variable. We end up separating the covariate space $\\mathcal{X}$ in $m$ regions given by $(R_1,... ,R_m)$. Then, the classifier is given by\n",
    "$$ f(x) = \\sum ^m_{j=1} v_j \\mathbb{1}_{x\\in R_j}. $$\n",
    "\n",
    "The $v_j$ is going to be the most voted class of the leaf.\n",
    "\n",
    "It is highly flexible but faces the inconvenience of potentially interpolating the entire dataset. To manage this, we tested multiple parameters, like the minimum number of samples required to split a leaf or the maximum allowed depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=DecisionTreeClassifier(random_state=seed)\n",
    "learning_testing(tree, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"criterion\":[\"gini\", \"entropy\", \"log_loss\"], \"max_depth\":[5, 10, 12, 15, 20, 30], \"min_samples_split\":[2, 5,10, 20], \"min_samples_leaf\":[1, 2, 5, 10]}\n",
    "tree = DecisionTreeClassifier(random_state=seed)\n",
    "GSTree = GridSearchCV(tree, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSTree.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSTree.best_params_} with a score of {GSTree.best_score_} for the model {GSTree.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=seed, criterion=\"entropy\", max_depth=30, min_samples_leaf=10, min_samples_split=2)\n",
    "weight.append(learning_testing(tree, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It aims to aggregate multiple decision trees as uncorrelated as possible. To do so,it leverages the bootstrap technique, involving resampling the data with replacement to simulate new datasets. A decision tree is trained for each bootstrap sample. The final predictor is determined by averaging the individual predictors:\n",
    "$$ \\hat f _{bag}(\\mathcal D_n)(x) = \\frac{1}{n} \\sum^b _{i=1}\\hat f_i(x). $$\n",
    "The primary advantage of this method is its reduction of variance while sustaining an equal level of bias. \n",
    "\n",
    " we also seek to enhance the method by fine-tuning multiple parameters, such as the number of estimators or the hyperparameters inherited by the decision tree, like the maximum depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier(random_state=seed, n_estimators=200, max_depth=20)\n",
    "learning_testing(RF, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_estimators\":[100,150, 250, 500], \"max_depth\":[10, 15, 20, 30, 50]}#, \"min_samples_split\":[2, 5,10, 20], \"min_samples_leaf\":[1, 2, 5, 10],\n",
    "RF = RandomForestClassifier(random_state=seed, max_features=3)\n",
    "GSRF = GridSearchCV(RF, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSRF.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSRF.best_params_} with a score of {GSRF.best_score_} for the model {GSRF.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=seed, max_features=3, max_depth=50, n_estimators=150)\n",
    "weight.append(learning_testing(RF, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network: Multilayer Perceptron Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network consists of a series of interconnected layers, each formed by multiple adapted weight combinations from the preceding layer. These combinations are activated by a function to introduce non-linearity, a key aspect that allows neural networks to act as universal approximators.\n",
    "\n",
    "Despite the opacity of these models, their consistent accuracy across numerous cases makes us consider them in our models.\n",
    "\n",
    "In this project, we focus on using the methods already implemented in the $\\textit{MLPClassifier}$ model of Scikit-learn. This includes optimizing essential parameters like the number and size of layers, choice of optimizers, activation functions, the number of iterations, and strategies for learning rate optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=MLPClassifier(random_state=seed, max_iter=500, hidden_layer_sizes=[100,100,100])\n",
    "learning_testing(mlp, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = { \"max_iter\":[400, 500, 600, 700], \"alpha\":[1e-3, 1e-4, 1e-5]}#\"max_iter\":[200, 500, 1000], \"learning_rate\":[\"constant\", \"invscaling\",\"adaptive\"], \"alpha\":[1e-3, 1e-4, 1e-5],\n",
    "parameters = { \"max_iter\":[400, 600, 1000], \"alpha\":[1e-5],\"activation\":[\"relu\",\"logistic\"], \"hidden_layer_sizes\":[[50,50, 40, 60], [100, 30, 30, 20, 20, 20,10]]}\n",
    "mlp = MLPClassifier(random_state=seed, solver=\"lbfgs\", learning_rate=\"constant\")\n",
    "GSmlp = GridSearchCV(mlp, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSmlp.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSmlp.best_params_} with a score of {GSmlp.best_score_} for the model {GSmlp.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=seed, solver=\"lbfgs\", activation=\"logistic\", learning_rate=\"constant\", alpha=1e-05, max_iter=600)\n",
    "weight.append(learning_testing(mlp, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an aggregation method, different from bagging, where the base predictors are not built in parallel and independently. Instead, they are constructed iteratively based on the residuals from previous iterations. \n",
    "The initial problem is given by $$\\widehat{f}\\in \\underset{(h_m, \\alpha_m)_{1\\leq m\\leq M}}{\\mathrm{argmin}}{\\frac{1}{n}\\sum_{i=1}^nc\\left(\\sum_{m=1}^M\\alpha_mh_m(X_i), Y_i\\right)}.$$\n",
    "However, this approach is not feasible. Then, as detailed in the report for the regression boosting, a simplification is proposed. In fact, with this simplification it is possible to compute an explicit iterative model for the exponential loss: Adaboost. \n",
    "However, we observe better score with log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradb=GradientBoostingClassifier(random_state=seed, n_estimators=300, max_depth=10, learning_rate=1)\n",
    "learning_testing(gradb, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \"n_estimators\":[100, 200, 300], \"learning_rate\":[0.1,0.5, 1], \"max_depth\":[5, 10, 15]}#\"criterion\":[\"friedman_mse\", \"squared_error\"],\n",
    "gradb = GradientBoostingClassifier(random_state=seed, loss=\"log_loss\", max_features=3)\n",
    "GSgradb = GridSearchCV(gradb, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSgradb.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSgradb.best_params_} with a score of {GSgradb.best_score_} for the model {GSgradb.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradb = GradientBoostingClassifier(random_state=seed, loss=\"log_loss\", max_features=3, learning_rate=0.5, max_depth=5, n_estimators=300)\n",
    "weight.append(learning_testing(gradb, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(gradb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are trying with XGboost and Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=XGBClassifier()\n",
    "learning_testing(xgb, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_estimators': [100, 200], 'max_depth': range(2,25)}\n",
    "xgb = XGBClassifier()\n",
    "GSxgb = GridSearchCV(xgb, params, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSxgb.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GSxgb.best_params_} with a score of {GSxgb.best_score_} for the model {GSxgb.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = GradientBoostingClassifier(random_state=seed, loss=\"log_loss\", n_estimators=200, max_depth=6, max_features='sqrt', learning_rate=0.1)\n",
    "weight.append(learning_testing(xgb, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostClassifier()\n",
    "learning_testing(cat, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'iterations':[300, 600], 'learning_rate':[0.05, 0.1,0.2], 'bootstrap_type':['Bayesian', 'Bernoulli']}\n",
    "cat = CatBoostClassifier()\n",
    "GScat = GridSearchCV(cat, params, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GScat.fit(X_final,  Y)\n",
    "print(f\"The best parameters are {GScat.best_params_} with a score of {GScat.best_score_} for the model {GScat.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostClassifier(bootstrap_type='Bayesian', iterations=600, learning_rate=0.2)\n",
    "weight.append(learning_testing(cat, X_train_pca, Y_train, X_test_pca, Y_test))\n",
    "models.append(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_train, agg_test = AggregatePredictions(models, X_train_pca, Y_train, X_test_pca, X_final, Y, test_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by selecting the label that received the most votes among the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_agg=agg_test.mode(axis=1).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(np.array(avg_agg), index=test.index).to_csv('data/predictions_avg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will calculate a $\\textit{weighted}$ mode to assign varying importance to different classifiers, taking into account their varying levels of accuracy. Our approach involves assigning each classifier a weight based on their F1-score, with the best classifiers receiving higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigth_mode(x):\n",
    "    accum=np.zeros(3)\n",
    "    for i in range(len(x)):\n",
    "        accum[x[i]]+=weight[i]\n",
    "    return np.argmax(accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_agg=agg_test.apply(weigth_mode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weighted F1 score: \\033[91m{f1_score(Y_test, weight_agg, average='weighted').round(6)}\\033[0m\")\n",
    "#pd.DataFrame(np.array(weight_agg), index=test.index).to_csv('weight.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same idea, we will train another method to better aggregate the base learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_estimators\":[100,200, 300, 350], \"max_depth\":[10, 20],\"min_samples_split\":[2, 5,10, 20], \"min_samples_leaf\":[1, 2, 5, 10] }\n",
    "RF = RandomForestClassifier(random_state=seed)\n",
    "GSRFag = GridSearchCV(RF, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSRFag.fit(agg_train,  Y_train)\n",
    "print(f\"The best parameters are {GSRFag.best_params_} with a score of {GSRFag.best_score_} for the model {GSRFag.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfAgpred=GSRFag.best_estimator_.predict(agg_test)\n",
    "print(f\"Weighted F1 score: \\033[91m{f1_score(Y_test, rfAgpred, average='weighted').round(6)}\\033[0m\")\n",
    "#pd.DataFrame(rfAgpred, index=test.index).to_csv('rfAgpred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \"max_iter\":[300, 500, 600, 800], \"alpha\":[1e-5]}#\"max_iter\":[200, 500, 1000], \"learning_rate\":[\"constant\", \"invscaling\",\"adaptive\"], \"alpha\":[1e-3, 1e-4, 1e-5],\n",
    "mlp = MLPClassifier(random_state=seed, solver=\"lbfgs\", activation=\"logistic\", learning_rate=\"constant\")\n",
    "GSmlp = GridSearchCV(mlp, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSmlp.fit(agg_train,  Y_train)\n",
    "print(f\"The best parameters are {GSmlp.best_params_} with a score of {GSmlp.best_score_} for the model {GSmlp.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpAgpred=GSmlp.best_estimator_.predict(agg_test)\n",
    "print(f\"Weighted F1 score: \\033[91m{f1_score(Y_test, mlpAgpred, average='weighted').round(6)}\\033[0m\")\n",
    "#pd.DataFrame(mlpAgpred, index=test.index).to_csv('data/predictions_mlpAgpred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prepare the data. Since our objective is diversity, we will introduce the initial data even before applying PCA to ensure that the $\\textit{base learners}$ are as uncorrelated as possible. Therefore, we achieve this uncorrelation by constructing the classifiers using different covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ag=pd.concat([X_test, agg_train], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ag = pd.DataFrame(index=range(X_test.shape[0]), columns=range(X_test.shape[1]+agg_train.shape[1]))\n",
    "X_ag.iloc[:, :X_test.shape[1]] = X_test.values\n",
    "X_ag.iloc[:, X_test.shape[1]:] = agg_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ag = pd.DataFrame(index=range(test.shape[0]), columns=range(test.shape[1]+agg_test.shape[1]))\n",
    "test_ag.iloc[:, :test.shape[1]] = test.values\n",
    "test_ag.iloc[:, test.shape[1]:] = agg_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_estimators\":[100,200,250, 275, 300, 350], \"max_depth\":[10, 20, 25, 30, 40],\"min_samples_split\":[2, 5,10], \"min_samples_leaf\":[1,  5, 10], \"max_features\":[3,4,5,6,8] }\n",
    "RF2 = RandomForestClassifier(random_state=seed)\n",
    "GSRFag2 = GridSearchCV(RF2, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSRFag2.fit(X_ag,  Y_test)\n",
    "print(f\"The best parameters are {GSRFag2.best_params_} with a score of {GSRFag2.best_score_} for the model {GSRFag2.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFAgpred2=GSRFag2.best_estimator_.predict(test_ag)\n",
    "#pd.DataFrame(RFAgpred2, index=test.index).to_csv('data/predictions_RFAgpred2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = { \"max_iter\":[300,400, 500, 600, 700, 900],\"alpha\":[1e-3, 1e-4, 1e-5], \"activation\": [\"logistic\", \"relu\"],\"learning_rate\":[\"constant\", \"invscaling\",\"adaptive\"]}#\"max_iter\":[200, 500, 1000], \"learning_rate\":[\"constant\", \"invscaling\",\"adaptive\"], \"alpha\":[1e-3, 1e-4, 1e-5],\n",
    "mlp2 = MLPClassifier(random_state=seed, solver=\"lbfgs\")\n",
    "GSmlp2 = GridSearchCV(mlp2, parameters, scoring=\"f1_weighted\", n_jobs=-1, verbose=3)\n",
    "GSmlp2.fit(X_ag,  Y_test)\n",
    "print(f\"The best parameters are {GSmlp2.best_params_} with a score of {GSmlp2.best_score_} for the model {GSmlp2.best_estimator_.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpAgpred2=GSmlp2.best_estimator_.predict(test_ag)\n",
    "#pd.DataFrame(mlpAgpred2, index=test.index).to_csv('data/predictions_mlpAgpred2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adopt a transductive perspective, treating the training set as our labeled data $\\mathcal{D}_n$ and the test set as the unlabeled data $\\mathcal{U}_m$. This approach assumes that the entire set we aim to predict is available from the outset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the already implemented heuristic of self-training that iteratively adds to the labeled data the unlabeled data for which we have a confident prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svc = SVC(probability=True, gamma=\"auto\")\n",
    "log=LogisticRegression(random_state=seed)\n",
    "self_training_model = SelfTrainingClassifier(log)\n",
    "X_semi=X.append(test)\n",
    "Y_semi=Y.append( pd.Series([-1] * test.shape[0]))\n",
    "stLog=self_training_model.fit(X_semi, Y_semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stLogR=stLog.predict(test)\n",
    "#pd.DataFrame(stLogR, index=test.index).to_csv('data/predictions_stLogR.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Self Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to build a robust version of the previous algorithm to add only the unlabeled data for which all the predictors where confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustSelfTrainingClassifior():\n",
    "    def __init__(self, Models):\n",
    "        self.models=Models.copy()\n",
    "\n",
    "    def fit(self, X, y, max_iter, seed=seed):\n",
    "        X_labeled, X_unlabeled, y_labeled, y_unlabeled=train_test_split(X, y, stratify=y, random_state=seed, test_size=0.25)\n",
    "        print(f'Initial labeled data: {len(X_labeled)}, Initial unlabeled data: {len(X_unlabeled)}')\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            # Train models\n",
    "            for name, model in self.models.items():\n",
    "                model['model'].fit(X_labeled, y_labeled)\n",
    "            \n",
    "            # Predict unlabeled data\n",
    "            Y_pred_dict = {}\n",
    "            for name, model in self.models.items():\n",
    "                pred = model['model'].predict_proba(X_unlabeled)\n",
    "                pred = pd.DataFrame(pred, index=X_unlabeled.index)\n",
    "                Y_pred_dict[name] = {'model_pred' : pred, 'tau': model['tau']}\n",
    "\n",
    "            # Create each set for each model\n",
    "            S = []\n",
    "            for name, model in Y_pred_dict.items():\n",
    "                model_pred = model['model_pred']\n",
    "                tau = model['tau']\n",
    "                S.append([set(model_pred.index[model_pred.iloc[:, i] > tau].tolist()) for i in range(int(y_labeled.min()), int(y_labeled.max())+1)])\n",
    "            # Get only the index of the unlabeled data which predict by all models\n",
    "            S = np.transpose(S)\n",
    "            S = [set.intersection(*item) for item in S]\n",
    "            S = set.union(*S)\n",
    "\n",
    "            # break the for if S is empty\n",
    "            if not S:\n",
    "                print('No unlabelled data unanimously predicted')\n",
    "                break\n",
    "\n",
    "            # Add the new labeled data to the labeled data\n",
    "            X_labeled = pd.concat([X_labeled, X_unlabeled.loc[list(S)]], axis=0)\n",
    "            # Delete the new labeled data from the unlabeled data\n",
    "            X_unlabeled = X_unlabeled.drop(list(S), axis=0)\n",
    "            # Add the new labeled data to the labeled data\n",
    "            y_labeled = pd.concat([y_labeled, y_unlabeled.loc[list(S)]], axis=0)\n",
    "            if X_unlabeled.shape[0] == 0:\n",
    "                print('All unlabeled data are labeled')\n",
    "\n",
    "            print(f'Iteration {i+1} finished, {len(X_labeled)} labeled data, {len(X_unlabeled)} unlabeled data')\n",
    "\n",
    "        else:\n",
    "            print('Max iteration reached')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # List of matrix prediction of each model\n",
    "        Y_list = []\n",
    "        for name, model in self.models.items():\n",
    "            Y_list.append(pd.DataFrame(model['model'].predict_proba(X), index=X.index))\n",
    "        # Average every prediction probability\n",
    "        Y = pd.concat(Y_list).groupby(level=0).mean()\n",
    "        # Get the index of the max probability\n",
    "        Y['target'] = Y.idxmax(axis=1)\n",
    "        # Get only the target column\n",
    "        Y = Y[['target']]\n",
    "\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindTau(model,  X_train, Y_train, X_val, Y_val):\n",
    "    \"\"\"\n",
    "    Find the best tau which maximize the f1 score\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: model to find the best tau\n",
    "    X_train: train data\n",
    "    Y_train: train labels\n",
    "    X_val: validation data\n",
    "    Y_val: validation labels\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_tau: best tau which maximize the f1 score\n",
    "    \"\"\"\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred = model.predict_proba(X_val)\n",
    "    pred = np.append(pred, np.zeros((pred.shape[0], 1)), axis=1).copy()\n",
    "    best_tau = 0\n",
    "    best_accuracy = 0\n",
    "    for tau in np.arange(0.5,1,0.025):\n",
    "        Y_pred = (pred >= tau).astype(int)\n",
    "        Y_pred[np.sum(Y_pred, axis=1) == 0, 3] = 1\n",
    "        Y_pred = np.argmax(Y_pred, axis=1)\n",
    "        accuracy = f1_score(Y_val, Y_pred, average='weighted')\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_tau = tau\n",
    "        #print(f\"tau: {tau:.2f}, accuracy: {accuracy:.3f}\")\n",
    "        \n",
    "    print(f\"Best tau for the model {model.__class__.__name__} is {best_tau}\")\n",
    "    return best_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_logistic = FindTau(lrNR, X_train_pca, Y_train, X_test_pca, Y_test)\n",
    "tau_cat = FindTau(cat, X_train_pca, Y_train, X_test_pca, Y_test)\n",
    "tau_RF = FindTau(RF, X_train_pca, Y_train, X_test_pca, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'logistic': {'model': lrNR, 'tau': tau_logistic},\n",
    "    'cat': {'model': cat, 'tau': tau_cat},\n",
    "    'RF': {'model': RF, 'tau': tau_RF},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSTC = RobustSelfTrainingClassifior(models).fit(pd.DataFrame(X_train_pca, index=Y_train.index), Y_train, max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weighted F1 score: \\033[91m{f1_score(Y_test, RSTC.predict(pd.DataFrame(X_test_pca, index=Y_test.index)), average='weighted').round(6)}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Robust Self-Training is not accurate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSTC = RobustSelfTrainingClassifior(models).fit(pd.DataFrame(X_final, index=Y.index), pd.DataFrame(Y), 50)\n",
    "RSTCpred=RSTC.predict(pd.DataFrame(test_trans))\n",
    "#pd.DataFrame(RSTCpred, index=test.index).to_csv('data/predictions_RSTCpred.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
